# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IEPbY6oKLPxQxQZNlJr_8-OOtnBB0cta
"""
import numpy as np
random_seed = np.random.seed(112)


import warnings
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')

import tensorflow as tf
tf.__version__

import keras
keras.__version__

import os 
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"  
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

from tensorflow.python.client import device_lib
device_lib.list_local_devices()

import os
from random import shuffle
import cv2


import keras
import keras.backend as K
from keras.preprocessing.image import ImageDataGenerator
#from keras.optimizers import Adam, SGD, RMSprop

#from keras.models import Model
from keras.utils import to_categorical

from keras import Sequential


from keras.layers import Input, Activation, merge, Dense, Flatten, Dropout, Reshape, Softmax, Conv2D, BatchNormalization, add, GlobalAveragePooling2D
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers.merge import concatenate
from keras.models import Model
from keras.applications.nasnet import NASNetLarge

from keras.optimizers import Adam

from keras.utils import plot_model
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import load_model

from keras.utils import np_utils

from keras.regularizers import l2

import pandas as pd

import matplotlib.pyplot as plt

output_classes = 3
batch_size = 32
img_height, img_width = 331, 331
input_shape = (img_height, img_width, 3)
epochs = 72

nb_train_samples = 16831
nb_validation_samples = 0
nb_test_samples = 231
val_split=0.2


def load_samples(csv_file):
    data = pd.read_csv(os.path.join('./',csv_file),delim_whitespace=True)
    data = data[['patient_id', 'filename', 'diagnosis', 'o1', 'o2']]
    file_names = list(data.iloc[:,1])
    # Get the labels present in the second column
    labels = list(data.iloc[:,2])
    samples=[]
    for samp,lab in zip(file_names,labels):
        samples.append([samp,lab])
    return samples

train_path = "/path/to/training/images/folder"
test_path = "/path/to/test/images/folder"
train_samples = load_samples("train_split_v3.txt")
test_samples = load_samples("test_split_v3_sel.txt")

print(train_samples[:5])
print(test_samples[:5])

nb_validation_samples = int(val_split*nb_train_samples)
shuffle(train_samples)
print("*"*50)
print(train_samples[:10])
val_samples = train_samples[:nb_validation_samples]
train_samples = train_samples[nb_validation_samples:]
nb_train_samples -= nb_validation_samples


def convert_label(label):
    if label=="normal":
       return 0
    elif label=="pneumonia":
       return 1
    else:
       return 2

def generator(samples,root_dir="",batch_size=32,shuffle_data=True,input_shape=(331,331,3)):
    """
    Yields the next training batch.
    Suppose `samples` is an array [[image1_filename,label1], [image2_filename,label2],...].
    """
    num_samples = len(samples)
    while True: # Loop forever so the generator never terminates
        if shuffle_data:
           shuffle(samples)

        # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]
        for offset in range(0, num_samples, batch_size):
            # Get the samples you'll use in this batch
            batch_samples = samples[offset:offset+batch_size]

            # Initialise X_train and y_train arrays for this batch
            X_train = []
            y_train = []

            # For each example
            for batch_sample in batch_samples:
                # Load image (X) and label (y)
                img_name = batch_sample[0]
                label = batch_sample[1]
                img =  cv2.imread(os.path.join(root_dir,img_name)) #,0)  # 0 for grayscale
                
                # apply any kind of preprocessing                
                resized = cv2.resize(img,(input_shape[0],input_shape[1]))
                img = resized
                # Add example to arrays
                X_train.append(img)
                y_train.append(convert_label(label))

            # Make sure they're numpy arrays (as opposed to lists)
            X_train = np.array(X_train)
            #print("X_train::shape::",X_train.shape)            
            y_train = to_categorical(y_train,num_classes=3)
            y_train = np.array(y_train)
            #print("y_train::shape::",y_train.shape)
            y_train = np.squeeze(y_train)
            # The generator-y part: yield the next training batch            
            yield X_train, y_train



#train_gen = generator(root_dir="/home/vision/muru/covid/COVIDNet/COVID-Net/data/train/",samples=train_samples,batch_size=batch_size)
#val_gen = generator(root_dir="/home/vision/muru/covid/COVIDNet/COVID-Net/data/train/",samples=val_samples,batch_size=batch_size)
test_generator = generator(root_dir=test_path,samples=test_samples,batch_size=batch_size,shuffle_data=False)


csv_logger = keras.callbacks.CSVLogger("/path/to/csv/logfile")
tensor_board = keras.callbacks.TensorBoard(log_dir='logs')
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)
check_pts = keras.callbacks.ModelCheckpoint("/path/to/checkpoint/file", monitor='val_loss', save_best_only=True, verbose=1)

callbacks_list = [tensor_board, check_pts, csv_logger]

layer_in = Input(shape=input_shape)

f1,f2_in,f2_out,f3_in,f3_out,f4_out= 64, 96, 128, 16, 32, 32
# 1x1 conv
conv1 = Conv2D(f1, (1,1), strides=(33,33), padding='valid', activation='relu')(layer_in)
# 3x3 conv
conv3 = Conv2D(f2_in, (1,1), strides=(33,33), padding='valid', activation='relu')(layer_in)
conv3 = Conv2D(f2_out, (3,3), padding='same', activation='relu')(conv3)

#NASNet Layer
Naslayer = NASNetLarge(include_top=False, weights='imagenet', input_shape=input_shape)(layer_in)

# 5x5 conv
conv5 = Conv2D(f3_in, (1,1), strides=(33,33), padding='valid', activation='relu')(layer_in)
conv5 = Conv2D(f3_out, (5,5), padding='same', activation='relu')(conv5)
# 3x3 max pooling
pool = MaxPooling2D((3,3), strides=(32,32), padding='valid')(layer_in)
pool = Conv2D(f4_out, (1,1), padding='same', activation='relu')(pool)
# concatenate filters, assumes filters/channels last
layer_concat = concatenate([conv1, conv3, conv5, Naslayer, pool], axis=-1)


x = layer_concat
x = GlobalAveragePooling2D()(x)
x = Dense(1024)(x)
x = BatchNormalization()(x)
x = Activation("relu")(x)
x = Dropout(0.5)(x)
x = Dense(1024)(x)
x = BatchNormalization()(x)
x = Activation("relu")(x)
x = Dropout(0.5)(x)
prediction = Dense(output_classes, activation="softmax")(x)


model = Model(inputs=layer_in,outputs=prediction)
#model.load_weights('weightsfile.h5')   #,compile=False)
model.summary()


adam_opt = Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-5)

model.compile(optimizer= adam_opt,
              loss = 'categorical_crossentropy',
              metrics=['accuracy'])


history = model.fit_generator(train_gen, 
                              steps_per_epoch= nb_train_samples // batch_size, 
                              epochs=epochs,
                              validation_data= val_gen,
                              validation_steps=nb_validation_samples // batch_size, 
                              callbacks=callbacks_list,                          
                              verbose=1)

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()



batch_size = 2
test_generator = generator(root_dir=test_path,samples=test_samples,batch_size=batch_size,shuffle_data=False)

score = model.evaluate_generator(test_generator, steps=nb_test_samples//batch_size)

print("metrics=",model.metrics_names)
print("score=",score)
print ('Test Score: ', score[0])
print ('Test Accuracy: ',score[1])

l=[i[1]  for i in test_samples]
truth = [convert_label(i[1]) for i in test_samples]
#print("truth is ",truth)
labels = ["normal","pneumonia","COVID-19"]
print("**"*30)
from collections import Counter
print(Counter(l))

batch_size = 1
test_generator = generator(root_dir=test_path,samples=test_samples,batch_size=batch_size,shuffle_data=False)
predicts = model.predict_generator(test_generator, steps=nb_test_samples//batch_size, verbose=1)
print("len of predicts::",len(predicts))
predict_class = np.argmax(predicts, axis=1)
errors = np.where(predict_class != truth)[0]
print("No of errors = {}/{}".format(len(errors),nb_test_samples))

from keras.utils.np_utils import to_categorical
from sklearn.metrics import confusion_matrix

from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix
from sklearn.metrics import classification_report, recall_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score


print('Accuracy {}'.format(accuracy_score(y_true=truth, y_pred=predict_class)))

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(truth,predict_class)

    
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report
y_pred = predicts
y_pred_probabilities=y_pred

y_pred = np.argmax(y_pred,axis = 1) 
y_actual = truth


confusion_mtx = confusion_matrix(y_actual, y_pred) 
#print(confusion_mtx)
target_names = labels
print(classification_report(y_actual, y_pred, target_names=target_names))

total=sum(sum(cm))


##################pretty print confusion matrix####################
from confusion_matrix_pretty_print import pretty_plot_confusion_matrix
from confusion_matrix_pretty_print import plot_confusion_matrix_from_data

plot_confusion_matrix_from_data(y_actual, y_pred, columns=["normal","pneumonia","COVID-19"])



####################################################################


############################ROC#####################################

from sklearn.metrics import roc_curve, auc
#from sklearn import datasets
#from sklearn.multiclass import OneVsRestClassifier
#from sklearn.svm import LinearSVC
#from sklearn.preprocessing import label_binarize
#from sklearn.cross_validation import train_test_split
import matplotlib.pyplot as plt

y_test = y_actual
y_score = y_pred_probabilities

n_classes = output_classes
y_test_n = []
y_score_n = []
for i in range(n_classes):
    oneVsAll = lambda j: 1 if j==i else 0 
    y_test_n.append([oneVsAll(j) for j in y_test])
    y_score_n.append([j[i] for j in y_score])


fpr = dict()
tpr = dict()
thresholds = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], thresholds[i] = roc_curve(np.array(y_test_n[i]), np.array(y_score_n[i]))
    roc_auc[i] = auc(fpr[i], tpr[i])

idToLabel = {0:"normal",1:"pneumonia",2:"covid-19"}
# Plot of a ROC curve for a specific class
for i in range(n_classes):
    plt.figure()
    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) %s' % (roc_auc[i], idToLabel[i]))
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()

####################################################################

#############################Sensitivity and Specificity#################

for i in range(n_classes):
    plt.figure()
    plt.plot(thresholds[i], 1-fpr[i], label = 'specificity')
    plt.plot(thresholds[i], tpr[i], label = 'sensitivity')
    plt.legend()
    plt.grid()
    plt.xlabel('Threshold value '+'for class '+idToLabel[i])
    plt.show()

############################################################################


from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix
from sklearn.metrics import classification_report, recall_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score

th = 0

acc = accuracy_score(truth,predict_class)
prec = precision_score(truth,predict_class,average=None)
f1 = f1_score(truth,predict_class,average=None)
recall = recall_score(truth,predict_class,average=None)


confusion_mtx = confusion_matrix(y_actual, y_pred) 
print(confusion_mtx)
target_names = labels
print(classification_report(y_actual, y_pred, target_names=target_names))

total=sum(sum(cm))


print('Accuracy::',acc)
print('Precision::',prec)
print('Recall::',recall)
print('F1::',f1)
